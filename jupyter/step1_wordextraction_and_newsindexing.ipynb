{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../py/')\n",
    "sys.path.append('../../soynlp/')\n",
    "import configuration as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fnames = config.normalized_corpus_fnames()\n",
    "corpus_indexs = config.normalized_corpus_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.414 Gb\n",
      "all cohesion probabilities was computed. # words = 125927\n",
      "all branching entropies was computed # words = 351956\n",
      "all accessor variety was computed # words = 351956\n",
      "2016-10-27_article_all_normed trained\n",
      "2016-10-27_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.138 Gb\n",
      "all cohesion probabilities was computed. # words = 127687\n",
      "all branching entropies was computed # words = 356498\n",
      "all accessor variety was computed # words = 356498\n",
      "2016-10-20_article_all_normed trained\n",
      "2016-10-20_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.130 Gb\n",
      "all cohesion probabilities was computed. # words = 124616\n",
      "all branching entropies was computed # words = 349041\n",
      "all accessor variety was computed # words = 349041\n",
      "2016-10-26_article_all_normed trained\n",
      "2016-10-26_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.146 Gb\n",
      "all cohesion probabilities was computed. # words = 106132\n",
      "all branching entropies was computed # words = 297133\n",
      "all accessor variety was computed # words = 297133\n",
      "2016-10-28_article_all_normed trained\n",
      "2016-10-28_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.075 Gb\n",
      "all cohesion probabilities was computed. # words = 66018\n",
      "all branching entropies was computed # words = 192404\n",
      "all accessor variety was computed # words = 192404\n",
      "2016-10-23_article_all_normed trained\n",
      "2016-10-23_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.072 Gb\n",
      "all cohesion probabilities was computed. # words = 48479\n",
      "all branching entropies was computed # words = 138429\n",
      "all accessor variety was computed # words = 138429\n",
      "2016-10-29_article_all_normed trained\n",
      "2016-10-29_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.105 Gb\n",
      "all cohesion probabilities was computed. # words = 103928\n",
      "all branching entropies was computed # words = 295063\n",
      "all accessor variety was computed # words = 295063\n",
      "2016-10-21_article_all_normed trained\n",
      "2016-10-21_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.104 Gb\n",
      "all cohesion probabilities was computed. # words = 125053\n",
      "all branching entropies was computed # words = 341779\n",
      "all accessor variety was computed # words = 341779\n",
      "2016-10-24_article_all_normed trained\n",
      "2016-10-24_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.122 Gb\n",
      "all cohesion probabilities was computed. # words = 121361\n",
      "all branching entropies was computed # words = 338773\n",
      "all accessor variety was computed # words = 338773\n",
      "2016-10-25_article_all_normed trained\n",
      "2016-10-25_article_all_normed tokenized\n",
      "\n",
      "training was done. used memory 1.062 Gb\n",
      "all cohesion probabilities was computed. # words = 47281\n",
      "all branching entropies was computed # words = 137755\n",
      "all accessor variety was computed # words = 137755\n",
      "2016-10-22_article_all_normed trained\n",
      "2016-10-22_article_all_normed tokenized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "def tokenize_corpus(scores, corpus, tokenized_corpus_fname):\n",
    "    tokenizer = LTokenizer(scores=scores)\n",
    "    with open(tokenized_corpus_fname, 'w', encoding='utf-8') as f:\n",
    "        for n_doc, doc in enumerate(corpus):\n",
    "            tf = Counter(tokenizer.tokenize(doc, remove_r=True))\n",
    "            tf = sorted(tf.items(), key=lambda x:x[1], reverse=True)\n",
    "            tf = ' '.join(['%s//%d' % t for t in tf])\n",
    "            f.write('%s\\n' % tf)\n",
    "            if (n_doc + 1) % 100 == 0:\n",
    "                sys.stdout.write('\\rtokenizing ... (%d in %d)' % (n_doc+1, len(corpus)))\n",
    "\n",
    "for corpus_fname in corpus_fnames:\n",
    "    corpus_name = corpus_fname.split('/')[-1][:-4]\n",
    "    \n",
    "    corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=True)\n",
    "    word_extractor = WordExtractor(min_count=10)    \n",
    "    word_extractor.train(corpus)    \n",
    "    \n",
    "    model_fname = '%s/%s.wordextraction' % (config.model_folder, corpus_name)\n",
    "    word_extractor.save(model_fname)\n",
    "    \n",
    "    score_fname = '%s/%s.scores' % (config.model_folder, corpus_name)\n",
    "    scores = word_extractor.word_scores()\n",
    "    with open(score_fname, 'wb') as f:\n",
    "        pickle.dump(scores, f)\n",
    "    \n",
    "    print('%s trained' % corpus_name)\n",
    "    \n",
    "    corpus.iter_sent = False    \n",
    "    cohesion = {word:score.cohesion_forward for word, score in scores.items()}\n",
    "    tokenized_corpus_fname = '%s/%s_cohesion_tokenized.txt' % (config.model_folder, corpus_name)\n",
    "    tokenize_corpus(cohesion, corpus, tokenized_corpus_fname)\n",
    "    \n",
    "    branching_entropy = {word:score.right_branching_entropy for word, score in scores.items()}\n",
    "    tokenized_corpus_fname = '%s/%s_rbe.txt' % (config.model_folder, corpus_name)\n",
    "    tokenize_corpus(branching_entropy, corpus, tokenized_corpus_fname)\n",
    "    \n",
    "    csbe = {word:(score.right_branching_entropy*score.cohesion_forward) for word, score in scores.items()}\n",
    "    tokenized_corpus_fname = '%s/%s_csbe.txt' % (config.model_folder, corpus_name)\n",
    "    tokenize_corpus(csbe, corpus, tokenized_corpus_fname)\n",
    "    \n",
    "    print('\\r%s tokenized\\n' % corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from konlpy.tag import Kkma, Twitter, Hannanum\n",
    "\n",
    "class KkmaWrapping:\n",
    "    def __init__(self):\n",
    "        self.kkma = Kkma()\n",
    "    def tf(self, doc):\n",
    "        words = Counter(self.kkma.pos(doc))\n",
    "        words = [('%s/%s'%(word, tag), freq) for (word, tag), freq in words.items() if tag[0] == 'N' or tag[0] == 'V']\n",
    "        return words\n",
    "    \n",
    "class TwitterWrapping:\n",
    "    def __init__(self):\n",
    "        self.twitter = Twitter()\n",
    "    def tf(self, doc):\n",
    "        words = Counter(self.twitter.pos(doc))\n",
    "        words = [('%s/%s'%(word, tag), freq) for (word, tag), freq in words.items() if tag == 'Noun' or tag == 'Verb' or tag == 'Adjective']\n",
    "        return words\n",
    "    \n",
    "class HannanumWrapping:\n",
    "    def __init__(self):\n",
    "        self.hannanum = Hannanum()\n",
    "    def tf(self, doc):\n",
    "        words = Counter(self.hannanum.pos(doc))\n",
    "        words = [('%s/%s'%(word, tag), freq) for (word, tag), freq in words.items() if tag[0] == 'N' or tag[0] == 'P']\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    import sys\n",
    "    sys.path.append('../py/')\n",
    "    sys.path.append('../../soynlp/')\n",
    "    import configuration as config\n",
    "    from soynlp.utils import DoublespaceLineCorpus\n",
    "    \n",
    "    corpus_fnames = config.normalized_corpus_fnames()\n",
    "    \n",
    "    from konlpy.tag import Kkma, Twitter, Hannanum\n",
    "    taggers = [('twitter', TwitterWrapping()), ('hannanum', HannanumWrapping()), ('kkma', KkmaWrapping())]\n",
    "    for name, tagger in taggers:\n",
    "        for corpus_fname in corpus_fnames:\n",
    "            corpus_name = corpus_fname.split('/')[-1][:-4]    \n",
    "            corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=False)\n",
    "            \n",
    "            tokenized_corpus_fname = '%s/%s_%s.txt' % (config.model_folder, corpus_name, name)\n",
    "            with open(tokenized_corpus_fname, 'w', encoding='utf-8') as f:\n",
    "                for n_doc, doc in enumerate(corpus):\n",
    "                    tf = tagger.tf(doc)\n",
    "                    tf = ' '.join(['//'.join(t) for t in tf])\n",
    "                    f.write('%s\\n' % tf)\n",
    "                    if (n_doc + 1) % 100 == 0:\n",
    "                        sys.stdout.write('\\rtokenizing ... (%d in %d)' % (n_doc+1, len(corpus)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
